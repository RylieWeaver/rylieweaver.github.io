**Why Reward Averaging Does Not Change the Optimal Policy**  
The optimal policy in RL is defined as the one that maximizes expected discounted returns:

$$
\pi^* \;=\; \arg \max_{\pi} \; \mathbb{E}_{w \sim \mathcal{W}} \Bigg[ 
   \sum_{t=1}^{T} \gamma^t \, r_t^{(w)} 
\Bigg],
$$

where the expectation is over target words $w \in \mathcal{W}$.  



### Why Reward Averaging Does Not Change the Optimal Policy

The optimal policy in RL maximizes expected discounted returns:
$$
\pi^* \;=\; \arg\max_{\pi}\; \mathbb{E}_{w\sim\mathcal{W}}
\Bigg[ \sum_{t=1}^{T} \gamma^{t}\, r_t^{(w)} \Bigg],
$$
where the expectation is over target words \(w \in \mathcal{W}\) and the prior is 1/|W|.

**Claim.** If at a state \(s\) we estimate the objective by sampling \(m\) words from \(\mathcal{W}\) uniformly (with or without replacement) and averaging their returns, the estimator is **unbiased**, so the **optimal policy is unchanged**.

**Proof.** Let
$$
G_\pi(w) := \sum_{t=1}^{T} \gamma^{t}\, r_t^{(w)}, \qquad
J(\pi) := \mathbb{E}_{W}\!\left[G_\pi(W)\right]
= \frac{1}{M}\sum_{w\in\mathcal{W}} G_\pi(w).
$$

Draw a (multi)set $S$ of size $m$ by sampling words uniformly from $\mathcal{W}$, and define the sample mean
$$
\widehat{J}_m(\pi) := \frac{1}{m}\sum_{w\in S} G_\pi(w).
$$

Write $C(u)$ for the number of times word $u$ appears in $S$. Then
$$
\widehat{J}_m(\pi)=\frac{1}{m}\sum_{u\in\mathcal{W}} C(u)\,G_\pi(u),
\qquad
\mathbb{E}[C(u)] = \frac{m}{M},
$$
(where $\mathbb{E}[C(u)]=m/M$ holds for sampling with replacement, and also without replacement by symmetry of simple random sampling). Therefore,
$$
\mathbb{E}\!\left[\widehat{J}_m(\pi)\right]
= \frac{1}{m}\sum_{u} \mathbb{E}[C(u)]\,G_\pi(u)
= \frac{1}{m}\sum_{u} \frac{m}{M}\,G_\pi(u)
= \frac{1}{M}\sum_{u} G_\pi(u)
= J(\pi).
$$

Hence $\widehat{J}_m(\pi)$ is an unbiased estimator of $J(\pi)$. Maximizing $\mathbb{E}[\widehat{J}_m(\pi)]$ is equivalent to maximizing $J(\pi)$; the argmax over policies is the same. Increasing $m$ reduces variance but does not change the optimum. $\square$


where m_i are sample from M (without replacement if m >= |M| and with replacement is m <= |M|).





